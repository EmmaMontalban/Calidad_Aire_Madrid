{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga librerias necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from pandas import concat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del los dataset correspondientes al año 2022\n",
    "Se cargan los csv de cada mes en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir('../data/aire/')\n",
    "file_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear único dataset con los los datos del año 2022\n",
    "Recorrer la lista files_names para crear un único dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aire = pd.DataFrame()\n",
    "for file_name in file_names:\n",
    "    df_current = pd.read_csv('../data/aire/' + file_name, delimiter=';')\n",
    "    print(file_name, df_current.shape)\n",
    "    df_aire = pd.concat([df_aire, df_current], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar los datos con el componente NO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_aire' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_aire \u001b[39m=\u001b[39m df_aire[(df_aire[\u001b[39m'\u001b[39m\u001b[39mMAGNITUD\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m9\u001b[39m)]\n\u001b[0;32m      2\u001b[0m df_aire\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_aire' is not defined"
     ]
    }
   ],
   "source": [
    "df_aire = df_aire[(df_aire['MAGNITUD'] == 9)]\n",
    "df_aire.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear una nueva columna FECHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aire['FECHA'] = pd.to_datetime(df_aire[['ANO','MES','DIA']].rename(columns = {'ANO': 'YEAR', 'MES': 'MONTH', 'DIA': 'DAY'}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer las datos de las medidas tomadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm2_5_columns = [ 'H01', 'H02', 'H03', 'H04', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'H24'] # feature\n",
    "veri_columns = [ 'V01', 'V02', 'V03', 'V04', 'V05', 'V06', 'V07', 'V08', 'V09', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24']\n",
    "\n",
    "hora_column = np.arange(1,25) # feature label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un dataset auxiliar con los datos extraidos  y las nuevas columnas creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_pm2_5 = pd.DataFrame(columns=['hora', 'dato', 'verificado'])\n",
    "aux = pd.DataFrame(columns=['ano', 'mes', 'dia', 'hora', 'dato', 'verificado', 'estacion']) # aux de próxima fila a ser añadida\n",
    "for  (h,d,v) in zip(hora_column, data_pm2_5_columns, veri_columns): \n",
    "     aux['dato'] = df_aire [ data_pm2_5_columns[h-1] ] \n",
    "     aux['verificado'] = df_aire [ veri_columns[h-1] ] \n",
    "     aux['hora'] = h-1\n",
    "     aux[['ano', 'mes', 'dia']] = df_aire[['ANO','MES','DIA']]\n",
    "     aux['estacion'] = df_aire ['ESTACION']\n",
    "     data_pm2_5 = pd.concat([data_pm2_5, aux], ignore_index=True)\n",
    "data_pm2_5 = data_pm2_5.astype({'hora': 'int32'}, copy = False)\n",
    "\n",
    "data_pm2_5.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadir la columna FECHA en formato datetime al nuevo dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm2_5['FECHA'] = pd.to_datetime(data_pm2_5[['ano','mes','dia','hora']].rename(columns = {'ano': 'year', 'mes': 'month','dia': 'day', 'hora': 'hour'}))\n",
    "data_pm2_5.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-index data frame con estacion y FECHA :O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm2_5.set_index(['estacion', 'FECHA'], inplace=True)\n",
    "data_pm2_5.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordenar por estación y FECHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm2_5 = data_pm2_5.sort_values(['estacion', 'FECHA'])\n",
    "data_pm2_5.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rellenar con NaN las filas perdidas \n",
    " Para cada estación, rellenamos con NaN si hay alguna fila perdida y guardamos el resultado en un nuevo dataframe que ya debería tener todas las filas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estaciones = data_pm2_5.index.get_level_values('estacion').unique()\n",
    "datetime_index = pd.date_range(start='1/1/2022', end='12/31/2022 23:00:00', freq='H', name='FECHA')\n",
    "data_pm2_5_nuevo = pd.DataFrame()\n",
    "for estacion in estaciones:\n",
    "    #data_pm2_5_estacion = data_pm2_5[data_pm2_5['estacion'] == estacion]\n",
    "    data_pm2_5_estacion = data_pm2_5.loc[estacion]\n",
    "    print(estacion, len(data_pm2_5_estacion))\n",
    "    data_pm2_5_estacion = data_pm2_5_estacion.reindex(datetime_index)\n",
    "    data_pm2_5_estacion.reset_index(inplace=True)\n",
    "    print(estacion, len(data_pm2_5_estacion))\n",
    "    data_pm2_5_estacion['estacion'] = estacion\n",
    "    data_pm2_5_nuevo = pd.concat([data_pm2_5_nuevo, data_pm2_5_estacion],ignore_index=True)\n",
    "    \n",
    "print(len(data_pm2_5_nuevo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-index data_pm2_5 frame con estacion y FECHA :O\n",
    "data_pm2_5_nuevo.set_index(['estacion', 'FECHA'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Actualizar  data con el nuevo df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm2_5 = data_pm2_5_nuevo\n",
    "data_pm2_5.info()\n",
    "data_pm2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los datos que se habían perdidos\n",
    "missing_data_pm2_5 = data_pm2_5[data_pm2_5['dato'].isna()]\n",
    "print(f'Cantidad de datos no existentes (perdidos): {len(missing_data_pm2_5)}')\n",
    "missing_data_pm2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos las columnas conciernientes a la FECHA que faltan en esos datos perdidos excepto la columna 'dato' que ya trataremos más adelante\n",
    "dt = missing_data_pm2_5.index.get_level_values('FECHA')\n",
    "data_pm2_5.loc[data_pm2_5['dato'].isna(),'ano'] = dt.year\n",
    "data_pm2_5.loc[data_pm2_5['dato'].isna(),'mes'] = dt.month\n",
    "data_pm2_5.loc[data_pm2_5['dato'].isna(),'dia'] = dt.day\n",
    "data_pm2_5.loc[data_pm2_5['dato'].isna(),'hora'] = dt.hour\n",
    "data_pm2_5 = data_pm2_5.astype({'hora': 'int32'}, copy = False)\n",
    "\n",
    "#data_pm2_5.loc[data_pm2_5['dato'].isna(),['ano', 'mes', 'dia', 'hora']] = [dt.year, dt.month, dt.day, dt.hour]\n",
    "data_pm2_5[data_pm2_5['dato'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 6. Por último, marcamos los datos no verificados como NaN\n",
    "no_verificados = data_pm2_5.loc[data_pm2_5['verificado'] != 'V']\n",
    "print(f'Cantidad de datos existentes pero no válidos:{len(no_verificados)}')\n",
    "data_pm2_5.loc[data_pm2_5['verificado'] != 'V', ['dato']] = np.nan\n",
    "data_pm2_5.loc[data_pm2_5['verificado'] != 'V']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borramos columna verificado\n",
    "del data_pm2_5['verificado']\n",
    "data_pm2_5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Primero de todo, deshacemos el multíndice\n",
    "data_pm2_5.reset_index(inplace=True) # Salvar los datos sin indices de estacion y FECHA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 7. Añadimos columna para el día de la semana\n",
    "#dates = data_pm2_5.index.get_level_values('FECHA').to_pydatetime() # con multiindex\n",
    "dates = data_pm2_5['FECHA'].dt.to_pydatetime() # con column y sin multiindex\n",
    "days_of_week = ['Lunes', 'Martes', 'Miercoles', 'Jueves', 'Viernes', 'Sabado', 'Domingo']\n",
    "data_pm2_5['dia_semana'] = np.array([days_of_week[d.weekday()] for d in dates])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm2_5 = data_pm2_5.rename(columns={'dato': 'PM2.5'}, copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_pm2_5.info()\n",
    "data_pm2_5.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm2_5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serie con solamente los datos de contaminación\n",
    "datos = data_pm2_5['PM2.5']\n",
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.to_csv('data_pm2_5_hist.csv') # Salvado serie solo datos históricos\n",
    "data_pm2_5.to_csv('data_pm2_5_FECHA.csv') # Salvado con los datos de las FECHAs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
